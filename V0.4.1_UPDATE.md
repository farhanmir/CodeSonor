# CodeSonor v0.4.1 - LLM Provider Expansion

## üÜï New LLM Providers Added

CodeSonor now supports **8 LLM providers** total! Added 3 new providers:

### 1. **OpenRouter** üåê
Access multiple LLM models through a unified API.

```bash
codesonor setup  # Select OpenRouter
# or
codesonor analyze URL --llm-provider openrouter --llm-api-key YOUR_KEY
```

**Supported Models:**
- `anthropic/claude-3-haiku` (default)
- `anthropic/claude-3-sonnet`
- `anthropic/claude-3-opus`
- `openai/gpt-4-turbo`
- `openai/gpt-3.5-turbo`
- `meta-llama/llama-3-70b-instruct`

**Get API Key:** https://openrouter.ai/keys

---

### 2. **xAI Grok** ü§ñ
Elon Musk's xAI Grok models.

```bash
codesonor setup  # Select xAI
# or
codesonor analyze URL --llm-provider xai --llm-api-key YOUR_KEY
```

**Supported Models:**
- `grok-beta` (default)

**Get API Key:** https://console.x.ai

---

### 3. **Ollama** üíª
Run LLMs locally on your machine - **no API key needed**!

```bash
# Install Ollama first
# Visit: https://ollama.ai/download

# Pull a model
ollama pull llama3

# Use with CodeSonor
codesonor setup  # Select Ollama
# or
codesonor analyze /path/to/local/repo --llm-provider ollama
```

**Supported Models:**
- `llama3` (default)
- `mistral`
- `codellama`
- `llama2`
- `phi`

**Benefits:**
- ‚úÖ Free and private
- ‚úÖ No internet required
- ‚úÖ Fast inference
- ‚úÖ No API costs

**Custom Base URL:**
```bash
# If Ollama runs on different host/port
codesonor analyze URL --llm-provider ollama --llm-api-key http://192.168.1.100:11434
```

---

## üìä Complete Provider List

CodeSonor now supports **8 LLM providers**:

| Provider | Type | API Key Required | Cost |
|----------|------|------------------|------|
| **Google Gemini** | Cloud | Yes | Free tier available |
| **OpenAI** | Cloud | Yes | Paid |
| **Anthropic Claude** | Cloud | Yes | Paid |
| **Mistral AI** | Cloud | Yes | Free tier available |
| **Groq** | Cloud | Yes | Free tier available |
| **OpenRouter** ‚≠êNEW | Cloud (Aggregator) | Yes | Pay-per-use |
| **xAI Grok** ‚≠êNEW | Cloud | Yes | Paid |
| **Ollama** ‚≠êNEW | Local | No | **FREE** |

---

## üîß Additional Changes

### Removed Email Placeholders
- Removed `your.email@example.com` from `__init__.py`
- Removed email from `pyproject.toml`
- Cleaner package metadata

### Updated Setup Wizard
- Now supports all 8 providers
- Dynamic provider selection (1-8)
- Provider-specific setup instructions
- Ollama gets base URL prompt instead of API key

### Updated Documentation
- All examples use generic placeholders
- Clear indication of what needs to be filled in
- Better instructions for each provider

---

## üöÄ Usage Examples

### Using OpenRouter
```bash
# Get API key from https://openrouter.ai/keys
export OPENROUTER_API_KEY="sk-or-v1-..."

codesonor analyze https://github.com/user/repo \
  --llm-provider openrouter \
  --llm-api-key $OPENROUTER_API_KEY \
  --llm-model "anthropic/claude-3-opus"
```

### Using xAI Grok
```bash
# Get API key from https://console.x.ai
export XAI_API_KEY="xai-..."

codesonor analyze https://github.com/user/repo \
  --llm-provider xai \
  --llm-api-key $XAI_API_KEY
```

### Using Ollama (Local)
```bash
# One-time setup
ollama pull llama3

# Analyze (no API key needed!)
codesonor analyze /path/to/repo \
  --llm-provider ollama
  
# Use different model
codesonor analyze /path/to/repo \
  --llm-provider ollama \
  --llm-model codellama
```

---

## üí° Why These Providers?

### OpenRouter
- **Access to 100+ models** through one API
- Pay only for what you use
- No need for multiple API keys
- Great for testing different models

### xAI Grok
- Latest model from xAI
- Competitive performance
- Alternative to OpenAI/Anthropic
- Real-time knowledge

### Ollama
- **100% FREE and private**
- No data sent to cloud
- Fast local inference
- Perfect for sensitive code
- Great for offline work
- No API rate limits

---

## üîÑ Migration Guide

### From v0.4.0 to v0.4.1

No breaking changes! All existing configurations work as-is.

To use new providers:

```bash
# Re-run setup to select new provider
codesonor setup

# Or use CLI flags
codesonor analyze URL --llm-provider ollama
```

---

## üéØ Best Practices

### When to Use Each Provider

**Ollama** - Best for:
- Analyzing private/sensitive code
- Offline work
- High-volume analysis (no API costs)
- Learning and experimentation

**OpenRouter** - Best for:
- Testing multiple models
- Pay-per-use pricing
- Access to latest models
- Comparing model outputs

**xAI Grok** - Best for:
- Real-time information needs
- Alternative to mainstream providers
- Latest model technology

**Cloud Providers** (Gemini/OpenAI/etc.) - Best for:
- Production use
- Highest quality analysis
- Specific model requirements
- API integration

---

## üì¶ Installation

```bash
# Upgrade to v0.4.1
pip install --upgrade codesonor

# Verify version
codesonor --version  # Should show 0.4.1

# For Ollama support
pip install requests  # Already included
```

---

## üêõ Bug Fixes

- Fixed email placeholders in package metadata
- Improved provider selection in setup wizard
- Better error messages for missing dependencies

---

## üìö Resources

- **OpenRouter Docs**: https://openrouter.ai/docs
- **xAI Docs**: https://docs.x.ai
- **Ollama Docs**: https://ollama.ai/docs
- **CodeSonor Docs**: See README.md

---

**Enjoy the new providers! üéâ**

Choose the one that fits your needs best - from free local LLMs to cutting-edge cloud models!
